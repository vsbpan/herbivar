---
title: Using *herbivar* to Fit Neutral Models
author: "Vincent Pan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document: 
    toc_float: yes
    toc: yes
    toc_depth: 2
    number_sections: yes
    code_folding: show
    highlight: tango
    theme: default
vignette: >
  %\VignetteIndexEntry{Using_herbivar_to_Fit_Neutral_Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
# vignette: >
#   %\VignetteIndexEntry{Using_herbivar_to_Fit_Neutral_Models}
#   %\VignetteEngine{knitr::rmarkdown}
#   %\VignetteEncoding{UTF-8}
```



```{r, include = FALSE}
knitr::opts_chunk$set(fig.align = "center", comment = NA, eval = TRUE)
```

```{r, include = FALSE}
# This document is being updated: 
# to do: 
# Motivation section in the intro explaining the use / application of the model 
# Biological interpretation of model 
# Make dalloT() more explicit
# Model output interpretation section. What to make of the parameter etc 
# Include worked example using real datasets. Explain how the package can be useful. -- Use image read from dames rocket photos
# Demo randomize leaves

```




# Introduction

|            The goal of this vignette is to introduce to the user on using the *herbivar* package to fit the neutral herbivory model (Pan et al. 2023) to empirical herbivory data and assess model fit. I also demonstrate some handy functions that can be used to characterize probability distributions and wrangle herbivory data more broadly. Finally, I provide some example applications of the neutral model. 

|            To follow this vignette and use the *herbivar* package, you'll need to first use the `install_github()` function from the *devtools* package to install the latest version of *herbivar* from github. You can just run the code chunk below on your machine to install the packages. The *herbivar* package depends on *EBImage*, which can be installed via `BiocManager::install()`. If you not using Windows, you might have a problem with the file `libX11.6.dylib`, which the dependent package *imager* uses. You'll need to install XQuartz (https://www.xquartz.org/) to fix the error. You can set `build_vignettes = TRUE` in `install_github()` if you want to have the vignette. 

```{r}
#install.packages("BiocManager")
#BiocManager::install("EBImage")
#install.packages("devtools")
if(suppressMessages(!require(herbivar))){
  devtools::install_github("vsbpan/herbivar", build_vignettes = FALSE, dependencies = TRUE)
} else {
  message("Sweet! Herbivar is installed!")
}

```


# Neutral herbivory model

## Formulation

|            The neutral herbivory model is a type of compound Poisson distribution, where the cumulative proportion herbivory $\phi_T$ with support over $[0,1]$ is a sum of $k$ individual 'bites' $\phi$ with a cut-off.  

$$
\phi_T = \Bigg\{ \begin{array}{cc}
     \begin{array}{cc}
      \sum_i^k \phi_i & \sum_i^k \phi_i \leq 1 \\
      1 & \sum_i^k \phi_i > 1
    \end{array}
\end{array}
$$


$k$ is drawn from a Poisson distribution with a fixed attack rate $\lambda$. The 'bite size' $\phi$ is not truly the amount of leaf area removed in a single bite, but the amount of leaf area removed in a single discrete and independent feeding bout that can be composed of multiple actual bites. I refer to it as 'bite size' for short. Using allometric scaling laws, Pan et al. (2023) argued that $\phi$ should follow the distribution (see the SI appendix of Pan et al. 2023 for more details):  

$$
P(\phi) = \Bigg\{ \begin{array}{cc}
     \begin{array}{cc}
      \frac{1}{\ln{\phi_M} - \ln{\phi_m}} \phi^{-1} & \alpha =1 \\
      \frac{1-\alpha}{\phi_M^{1-\alpha} - \phi_m^{1-\alpha}} \phi^{-\alpha} & \alpha \neq 1
    \end{array}
\end{array}
$$
Here, $\phi_M$ and $\phi_m$ are the maximum bite size and minimum bite size, set at 1 and 0.005 by default mostly out of convenience. $\alpha$ is a combined allometric scaling coefficient derived to be roughly $\frac{14}{9}$. It is important to note that this bite size distribution is equivalent to the well-known truncated Pareto distribution,  

$$
P(x) = \frac{\gamma x_{min}^\gamma x^{-\gamma-1}}{1-(\frac{x_{min}}{x_{max}})^\gamma},
$$
 where $\gamma = \alpha  + 1$, $x_{max} = \phi_M$, and $x_{min} = \phi_m$.   
 

## Supporting functions

|            The density function, distribution function, quantile function, and random generation of the 'bite size' distribution can be accessed via `dallo()`, `pallo()`, `qallo()`, and `rallo()` respectively. The exact mean and variance can be accesses with `get_phi_bar()` and `get_phi_var()` provided that the correct distribution parameters are specified.  

|            The density function, distribution function, quantile function, and random generation of the neutral herbivory distribution can be accessed via `dalloT()`, `palloT()`, `qalloT()`, and `ralloT()` respectively. The calculations here are much less trivial as the neutral herbivory distribution does not have a closed form solution. As such, we must resort to numerical approximation for the density, distribution, and quantile function. For the quantile function, a default of 1000 draws from the neutral herbivory distribution are generated and the quantiles are calculate empirically relative to these 1000 draws. The distribution function is calculated via numeric integration of the density function at a default resolution of 0.001. The density function is calculated via a truncated numeric summation:  

$$ 
P(\phi_T | \lambda) \approx \sum_{k=0}^{k_{max}} P(\phi_T|k)P(k|\lambda)
$$ 
$k_{max} = 50$ by default as with sufficiently large $k$, $P(k|\lambda)$ becomes vanishingly small. The function throws a warning when the error tolerance set by the argument `k.max.tolerance` is exceeded, at which point the user should increase the default `k.max` setting. The calculation of $P(\phi_T|k)$ is trickier and involves utilizing the convolution theorem and finding the $k^{th}$ power of the Fourier transformed $P(\phi)$ function. To avoid overflow and improve computational efficiency, convolutions with $k > 100$  are approximated with a normal distribution instead, given the central limit theorem. This limit can be toggled via the `k.fft.limit` setting. More details of the calculation can be found in the help file (`?dalloT`) or the SI appendix of Pan et al. (2023). Broadly speaking, the important thing to note is that increasing the simulation resolution via the arguments `n.sim` or `by` increases the precision of the estimates. This can become important when fitting models to data, the optimizer can fail to find the likelihood peak, yielding inaccurate maximum likelihood estimates and convergence issues as a result. Reducing the resolution below the default values is generally not recommended.    

|            The computational challenge of increasing the numerical approximation resolution is partially alleviated by the parallel computing feature implemented in *herbivar*. This feature can be access via functions with the argument `cores` and `parallel`. Setting `prallel = TRUE` and `cores` above 1 would enable the feature when the function detects that computational efficiency gain may be acquired. In general, functions like `dalloT()` are fast enough to not gain that much from parallel computing, unless the numerical approximation solution is set very high (0.00001 < by). Even then, the gain in efficiency is only very modest. However, this feature would become important when fitting the neutral model to data in which repeated evaluations of the log likelihood function is required, but the overhead of setting up the parallel sessions is only paid once. 

```{r}
# Generate some numbers
x <- ralloT(10000, lambda = 3)

# Check number of computer cores available
parallel::detectCores()

# Compute log likelihood with 4 computer cores
# Compute the same expression without parallelization
# microbenchmark::microbenchmark(
#   "no parallel" = sum(dalloT(x, lambda = 1, log = TRUE, by = 0.000001, parallel = FALSE)),
#   "with parallel" = sum(dalloT(x, lambda = 1, log = TRUE, by = 0.000001,
#                                parallel = TRUE, cores = 4)),
#   times = 1)
# Unit: seconds
#           expr      min       lq     mean   median       uq      max neval
#    no parallel 27.10942 27.10942 27.10942 27.10942 27.10942 27.10942     1
#  with parallel 24.49700 24.49700 24.49700 24.49700 24.49700 24.49700     1

```
  
  
|            An additional challenge of the neutral herbivory model is that its mean is also not trivial to calculate. However, with a slight reparameterization, the model parameter $\lambda$ can be rewritten as $\overline{\phi'}$ with support $[0,\infty]$ that is very close to the true mean.  

$$
\overline{\phi'} = \overline\phi \lambda
$$
For this reason, I opted to write the package using the $\overline{\phi'}$ parameterization by default. However, users can easily convert between $\lambda$ and $\overline{\phi'}$ using the functions `get_mean_phi_T()` and `get_lambda()` provided that the other model parameters are also specified (see `?get_phi_bar()`). Users can also use the `lambda` argument over the `mean.phi.T` argument in supported functions.  

```{r}
# Compute density function using mean.phi.T paramterization or lambda parameterization
dalloT(0.5, mean.phi.T = 0.1, 
       min.phi = 0.01, max.phi = 1, a = 1.5)
dalloT(0.5, lambda = get_lambda(mean.phi.T = 0.1, 
                                min.phi = 0.01, max.phi = 1, a = 1.5), 
       min.phi = 0.01, max.phi = 1, a = 1.5)
```

We can see that the results are identical.  

```{r}
# Get the 0.1 to lambda and back
get_mean_phi_T(
  get_lambda(mean.phi.T = 0.1, 
             min.phi = 0.01, max.phi = 1, a = 1.5), 
  min.phi = 0.01, max.phi = 1, a = 1.5)
```


## Fitting the neutral herbivory model

|            Currently, the neutral herbivory model can be fitted to data using maximum likelihood estimation through the wrapper `fit_allo_herb()`. Users have the option to fit various parameters to data using the argument `optim.vars`. Default model parameter values that are not fitted can be specified through the argument `param.vals`. If convergence fails, users can change the optimizer via the argument `method`, in addition to changing the numerical approximation resolution via `by`, and/or bounds of optimization via `upper` and `lower`. The argument `id` can be supplied with a character string that serves as a unique identifier of the data set for book keeping.  

```{r}
# Generate some fake herbivory data
set.seed(12)
x <- ralloT(1000, mean.phi.T = 0.123)


allo_fit <- fit_allo_herb(x, method = "Brent") # By default, mean.phi.T is fitted
allo_fit

# Fit two variables at once
allo_fit2 <- fit_allo_herb(x,
              optim.vars = c("mean.phi.T","a"),
              init = c(10, 1),
              by = 0.001,
              method = "Nelder-Mead",
              cores = 1, # setting cores > 1, enables parallel
              id = "This lablel is passed to the object") 
allo_fit2
```

The returned object has the class `allo_herb_fit` and is currently supported by some S3 generic methods, including `coef()`, `logLik()`, `AIC()`, `AICc()`, `BIC()`, `predict()`, and `print()`.  


```{r}
class(allo_fit)
```


|            More information can be retrieved from the model object (see `?fit_allo_herb()` for more details). Of particular note is that `par` and `se` give you the raw estimates of the MLE parameters that are on a transformed scale as shown in `param.val.trans`. To retrieve the model coefficients, the user should use `coef()` or `print()`. Keep in mind that the standard errors (as well as the 95% confidence intervals) from both functions are approximated via the first order second moment method (see `?FOSM()`) by default. 

```{r}
names(allo_fit)
coef(allo_fit, se = TRUE)
```


# Generic null models
## Formulation

|            In this section, I demonstrate how to fit generic non-neutral non-processes based null models to herbivory data. This becomes important in a later section where I discuss how to compete the neutral herbivory model against the generic null models. Currently, two null distributions are implemented in *herbivar*: Zero-One-Inflated-Beta distribution (ZOIB) and Hurdle Truncated Log-Normal distribution (HTLN). An alternative non-parametric way of generating a null distribution can be accessed via `randomize_leaves()`.  

|            The zero-one-inflated beta distribution has the density function (Ospina & Ferrari 2008), 

$$
P(x; p_0, p_1,\alpha,\beta) = \Bigg\{ \begin{array}{cc}
     \begin{array}{cc}
      p_0 & x = 0 \\
      p_1 (1-p_0) & x = 1 \\
      (1-p_0)(1-p_1)\frac{x^{\alpha-1}(1-x)^{\beta - 1}}{B(\alpha,\beta)} & x \neq 0,1
    \end{array}
\end{array}
$$

It can be considered as an extension to the beta distribution commonly used to model proportion herbivory. The parameters $\alpha$ and $\beta$ correspond to the same parameters in a beta distribution. The additional parameters $p_0$ and $p_1$, model the probability of zeros and ones. Using this extension properly models the presence of 0s and 1s in proportion herbivory data without the need for arbitrary transformations (Smithson & Verkuilen 2006, Warton & Hui 2011). Still, in practice, the large number of family parameters and computational limitations may render these transformations more desired. Users can use the function `adjust_prop()` to select from a list of transformation methods. The recent ordered beta regression of Kubinec (2022), which models the bounds as censored sections of a single continuous latent variable is another nice solution. It is available as `family = ordbeta()` in the *glmmTMB* package. It should be noted however, that when a single mean is estimated (i.e. intercept only model), the ordered beta regression is equivalent to a reparamterized ZOIB.  

|            The hurdle-truncated-log-normal distribution has the density function, 
$$
P(x;\theta, \mu, \sigma) = \Bigg\{ \begin{array}{cc}
     \begin{array}{cc}
      \theta & x = 0 \\
      (1-\theta)(1-G(x = 1; \mu, \sigma)) & x = 1 \\
      (1-\theta)g(x,\mu,\sigma) & x \neq 0,1
    \end{array}
\end{array}
$$
and can be considered as an extension to the log-normal distribution with density function $g(x;μ,σ)$ and cumulative distribution function $G(x;μ,σ)$. The additional parameter $\theta$ models the probability of zeros and values drawn from the log-normal distribution above one are truncated to one. The extension again allows the distribution to have probability mass at zero and one. The log-normal distribution is a reasonable candidate distribution for its ability to generate high positive skew in addition to being a limiting distribution for repeated multiplicative processes.


## Supporting functions

|            For the ZOIB distribution, the density function and random number generation can be accessed via `dzoibeta()` and `rzoibeta()` respectively. For the HTLN distribution, the density function and random number generation can be access via `dhtlnorm()` and `rhtlnorm()` respectively.  


## Fitting the generic null models

|            Both the ZOIB and HTLN distributions can be fitted to data using maximum likelihood estimation via the function `fit_generic_null()`. Again, the user has the ability to toggle the arguments `init`, `method`, `upper`, and `lower` to mess with the optimizer in case of convergence issues. The outputs and other arguments work pretty much the same way as `fit_allo_herb()`.  

```{r}
# Generate some fake herbivory data
set.seed(12)
x <- ralloT(1000, mean.phi.T = 0.123)

htlnorm_fit <- fit_generic_null(x, family = "htlnorm", method = "BFGS", id = "hmmmmmm")
htlnorm_fit


zoibeta_fit <- fit_generic_null(x, family = "zoibeta", method = "Nelder-Mead")
zoibeta_fit


```

The returned object has the class `generic_null_fit` and is currently supported by some S3 generic methods, including `coef()`, `logLik()`, `AIC()`, `AICc()`, `BIC()`, `predict()`, and `print()`.  


# Model assessment 

|            In this section, I show how one can use the package to assess model fit. There are numerous ways to determine how well a probability distribution fit to data, but they can be loosely divided into two types: absolute and relative measures of model fit. Absolute measures of model fit compare a model to data using objective measures. They inform how closely a model approximate observed data. Conversely, relative measures of model fit compete a model against alternative null hypotheses, allowing for strong inference (Platt 1964). McGill (2003) and McGill et al. (2006) provided excellent discussion on this important distinction for inference. I will just briefly emphasize that the two measures of model fit achieve slightly different goals. If you are interested purely in approximation or prediction, examining absolute measures of fit is sufficient. However, if you are interested in testing hypotheses, you need to examine the relative fit of a candidate model against other null or alternative hypotheses. In general, there is some burden of proof in ecology when one asserts the truth of a hypothesis, and we often show evidence through hypothesis significance testing. Examining absolute measures of model fit is still important, but not sufficient, because all alternative hypotheses can fit the data well on absolute measures or generate identical predictions. Therefore, tests based on absolute measures of fit maybe unable to falsify any hypothesis.  

## Absolute measures of fit

### Two-sample tests

|            The *herbivar* package implements a couple of methods to compare whether two samples are drawn from the same distribution and can be accessed via the function `compare_dist()`. Users can select the method using the `test` argument. Setting `test = "ks"` performs the Kolmogorov-Smirnov test using `stats::ks.test()`. It is a non-parametric test that looks at whether the maximum distance between two empirical cumulative distribution functions exceed expectation. The test statistic corresponds to the maximum distance. Setting `test = "ad"` performs the Anderson-Darling test using `kSamples::ad.test()`. The test looks at whether the empirical cumulative distribution function of a sample is uniform. Because the test is a permutation test, the computation can be quite intensive. "kl" computes the Kullback-Leibler divergence using `philentropy::KL()`. The unit of the KL divergence is "log" by default. It is a measure of relative entropy and can be interpreted as the expected additional amount of information needed to have the neutral model fully agree with observed data. Setting `test = "chisq"` performs the Chi-square test using `stats::chisq.test()`. In general, the Chi-square test should be avoided as it is quite sensitive to the bin size and does not preserve information on the order of the bins. 

```{r}
compare_dist(data_list = list("obs" = allo_fit$data, 
                              "pred" = predict(allo_fit)),
             test = c("ks", "kl","ad","chisq"), 
             bin_size = 0.01
            )
```


### Utilizing statistical 'probes'

|            While the above methods (except for KL divergence) can be used to test whether the observed and predicted distributions are distinguishable, when the methods yield a significant deviation, little can be learned about how two distributions are different. For this reason, we may look towards the classic paper by Kendall et al. (1999) who proposed using 'probes' as an objective measure of model performance. While the paper is about analyzing time series, the more general approach of using 'probes' to characterize the key features of data that we care about is tremendously useful. In this case, we want to use summary statistics of distributions as 'probes' and examine how closely two distributions agree on the values of those 'probes'. We already do this frequently when we compare the mean of two distributions. Occasionally, we compute other summary statistics (e.g. CV, variance) and examine how closely two distributions agree on those single static, using measures such as via RMSE or R^2. This is great if we only care about those particular 'probes'. However, when we examine whole distributions, we care about much more than the agreement of single summary statistic. We want to learn about the mean, variance, skewness, kurtosis, minimum, maximum, and etc. Because many of these summary statistics exhibit constrained relationships (e.g. distributions that follows Taylor's law has a positive mean-variance relationship, the kurtosis-skewness relationship is bounded by an inequality), it is better to treat all summary statics as following a multivariate distribution. We can then use conventional multivariate methods to examine this multivariate distribution. I show an example of an analysis using redundancy analysis (RDA) below. 


|            The `probe_dist()` function in *herbivar* allows you to calculate the value of statistical probes of a vector of data. Users can specify any number of probes that can be matched via R's `match.fun()`.
```{r}
data <- ralloT(1000, lambda = 3)
probe_dist(data, probes = c("mean", "var", "Skew","median"))
```

|            Here, I simulated two different distributions (one with $\phi_M = 1$ and the other with $\phi = 0.5$). Each distribution is replicated 100 times, each time drawing 1000 samples. For each replicated distribution, I used `probe_dist()` to calculate some summary statistics. 
```{r}
#Simulate distributions
allo_probed <- lapply(seq_len(100), function(i){
  x <- ralloT(1000, lambda = 3, max.phi = 1)
  probe_dist(x)
}) %>%
  do.call("rbind",.)

allo_probed2 <- lapply(seq_len(100), function(i){
  x <- ralloT(1000, lambda = 3, max.phi = 0.5)
  probe_dist(x)
}) %>%
  do.call("rbind",.)

# Bind everything together and create a new column that keeps track of how each sample is generated
probed.data <- rbind(
  data.frame("distribution" = "max.phi = 1", allo_probed),
  data.frame("distribution" = "max.phi = 0.5", allo_probed2)
)
```

  
  
|            Then, using the *vegan* package, I can use principal component analysis to see that there is a clear separation of samples generated from the two distributions. The loading scores can be used to find that the two distributions differ most in terms of the kurtosis.  

```{r}
vegan::rda(probed.data[,-1]) %>%
  get_biplot(choices = c(1,2), 
             group = probed.data$distribution) + 
  ggplot2::theme_bw(base_size = 15) + 
  ggplot2::labs(color = "Distribution")
```


|            Here, I use the distribution as a predictor in a redundancy analysis. We can see that the a substantial proportion of variance in the summary statics can be explained by the predictor (how the distributions are generated). This is clear support that the two distributions are different. Further analysis can be done using the *vegan* package which I will not go into here except that the significance of the predictor can be test via `anova()`. 

```{r}
vegan::rda(probed.data[,-1] ~ probed.data$distribution)
```


## Relative measures of fit

|            Here, I show how one can compete a candidate model against multiple alternative hypotheses. Using the fitted objects from `fit_allo_herb()` and `fit_generic_null()`, we can do likelihood based model selection through the functions `AIC()`, `AICc()`, `BIC()`, and `LRT()`. These functions are also compatible with any model objects for which the log likelihood and other information can be extracted through the generic method `logLik()`. 

|            The likelihood ratio test performed via `LRT()` tests whether a candidate model has a greater likelihood than a reference model. It is important to keep in mind that the candidate model should be nested within the reference model. As below, the two models are not nested, so the function throws a warning. 
```{r}
# Likelihood ratio test
# The HTLN model is the candidate model and the neutral herbivory model is the reference model
LRT(model_candidate = htlnorm_fit,
    model_ref = allo_fit)
```



|            A more versatile method is to use information criterion to compare models. This method allows the degrees of freedom of the null model to be equal or greater than the candidate model. The model complexity penalty is by default 2, but can be changed via the argument `k`. We can see here that the neutral model fits the data best as we generated the data from the same distribution. 

```{r}
# Akaike's Information Criterion
AIC(allo_fit) # extract single criterion
AIC(htlnorm_fit, allo_fit, zoibeta_fit) # Or provide multiple models to get a table

# Akaike's small sample size Corrected Information Criterion
AICc(allo_fit)

# Bayesian information criterion
BIC(allo_fit)

```


# Miscellaneous
## Batch processing

|            When dealing with many data sets or many models fitted to many data sets, one may desire to perform one action across all data sets or all models. This is made easier by batch processing wrappers implemented in *herbivar*. To use these functions, in general, multiple data sets or multiple models of the same class should be stored in a named list. The list may be named by some identifier, such as the data set ID or survey ID.  
|  
|  
For instance, `predict_list()` is a batch processing wrapper for `predict()`. 
```{r}
predict_list(list("a" = allo_fit,
                  "b" = allo_fit,
                  "c" = allo_fit), 
             n.sim = 3, 
             return.obs = FALSE)
```



  
The function `compare_dist_list()` is a batch processing wrapper for `compare_dist()` and supports doing bootstrap simulations.   
```{r}
compare_dist_list(list("a" = allo_fit,
                       "b" = allo_fit,
                       "c" = allo_fit),
                  test = c("ks","ad","kl"), 
                  nboot = 2, 
                  silent = TRUE) 
```



  
The `probe_dist_list()` function is a batch processing wrapper for `probe_dist()`. 

```{r}
probe_dist_list(
  list("a" = x,"b" = x, "c" = x), 
  probes = c("mean", "Hoover", "J.index", "cd", "cv", "N","lac","Gini")
)
```



  
Histogram and empirical cumulative distribution function plots can be easily made with the function `plot_distributions()`. 

```{r}
plot_distributions(
  purrr::map(predict_list(list(allo_fit)), 1)
) %>% suppressMessages()
```


## Fit 'bite size'
  
|            Finally, the *herbivar* package supports fitting observed 'bite size' distribution to seven different candidate models. More details can be found in `?bite_size_fit()`. 

```{r}
set.seed(12)
z <- rallo(100, max.phi = 0.4)
bite_size_fit <- fit_bite_size(z, 
                               min.phi = 0.005, 
                               family = "all")
bite_size_fit
```

Currently, the following generic methods are supported for the fitted object: `coef()`, `print()`. 

# Applications

## Benchmarking your $r^2$

|            Here, I show how one can use the neutral herbivory model to conduct simulations to benchmark observed $r^2$ of herbivory from experiments. The motivation is that in ecology, we tend to observe very low variance explained ($r^2$) and it is unclear how to interpret the size of the magnitude itself. For instance, a meta-analysis by Moller and Jennions (2002) reports most studies have an $r^2$ between 1.99 and 7.05%. This might be expected if the process we are measuring has a lot of baked in process variance. For instance, flipping a set of $n$ perfectly fair coins ($p = 0.5$) once would yield a variance of $0.25n$ in the number of heads. If we let the set of coins  differ in their probability of getting heads (i.e. $\mathbb{Var}[P] > 0$), we would see greater variance in the number of heads $H$, but some of the total variance would still be from the process itself. More precisely, $$\mathbb{Var}[H] = \mathbb{Var}[\mathbb{E}[H|P]] + \mathbb{E}[\mathbb{Var}[H|P]].$$ 

If as functional ecologists, we wish to explain the variation of the number of heads with the variation of the quality of the coins, we would expect to obtain an $r^2$ value of $$r^2 = \frac{ \mathbb{Var}[\mathbb{E}[H|P]]}{\mathbb{Var}[H]}.$$ 

This is very nice because if we know (or assume) the data generating process and have a good prior for how much variability there is in trait mediated effect on our variable of interest (i.e. $\mathbb{Var}[\mathbb{E}[H|P]]$), we can calculate the maximum proportion of variance that can be explained (upper bound of $r^2$). This upper bound can then be used as a bench mark to understand the actual $r^2$ you observe in a study. 

|            For example, let there be two equally abundant plant phenotypes in a landscape. One phenotype has a high rate of being attacked $\lambda = 1$, while the other phenotype has a low rate of being attacked $\lambda = 0.1$. For most ecologically realistic scenarios, this is a pretty big effect size (Carmona et al. 2011). If we assume that the neutral herbivory model approximate the data generating process well, then we can simulate the maximum proportion variance explained due to traits. In this case, if a trait we study (e.g. leaf N content) has a marginal $r^2$ of 3%, that is already roughly half of all the variance that can be explained! 

```{r}
# High lambda phenotype
lambda_h <- 1

# Low lambda phenotype
lambda_l <- 0.1

# n simulations
n <- 1000

# Simulate herbivory as one would observe them in the field
set.seed(100)
y <- c(ralloT(n, lambda = lambda_h), ralloT(n, lambda = lambda_l))

# Simulate phenotype data
x <- c(rep(lambda_h, n), rep(lambda_l, n)) 

# r2 benchmark
var(predict(lm(y~x))) / var(y)

```




## Detection of unexpected deviations

|            Here, I show how one can detect unexpected deviations from neutrality in two real herbivory data sets. The example data sets can be accessed through `herbivar::herb_data_example`. The first data set (`field`) comes from 200 *Hesperis matronalis* leaves I collected along the North Country Trail around Kellogg Biological Station in June. The second data set (`lab`) comes from 100 artificial agar blocks I fed to a single *Spodoptera exigua* caterpillar. In both cases, I scanned the leaves or agar blocks and used the `leaf_herb()` function to measure the proportion chewing damage. The minimum non-zero proportion herbivory is 0.5%. Because the regional herbivore pool is limited, we expect to see greater deviation of the `lab` herbivory distribution from the neutral herbivory distribution.  

```{r}
# Histogram of included example data set
plot_distributions(herb_data_example, by = 0.05,type = "hist")

```


|            Here, I fitted the neutral herbivory distribution to each data set then calculated the value of statistical probes for each data set. I repeated this for the observed distribution and 100 predicted distributions generated from the fitted neutral model.  


```{r}
# Fit neutral model for each data set
lab_herb_fit <- fit_allo_herb(herb_data_example$lab, method = "Brent")
field_herb_fit <- fit_allo_herb(herb_data_example$field, method = "Brent")

# Select some interesting statistical probes
probes <- c("mean","var","max","Skew","Kurt","cv","Gini","lac")

# Get value of probes of the raw data sets and 100 predicted distributions each
set.seed(123)
probes_d <- probe_dist_list(c(lapply(seq_len(100), function(x){
  predict(field_herb_fit)
}),
list(field_herb_fit$data),
lapply(seq_len(100), function(x){
  predict(lab_herb_fit)
}),
list(lab_herb_fit$data)),
probes = probes) %>%
  cbind("source" = c(rep("field", 101), rep("lab", 101)),
        "type" = rep(c(rep("predicted", 100), "observed"), 2)
  )

```


As we can see, the two distributions (red vs blue) separately nicely in the first two principal components. More interestingly, we can see that the observed distribution of the `lab` data set is very different from the neutral distributions as we expect (circles = observed; triangles = neutral prediction). On the other hand, the observed distribution of the `field` data set appears indistinguishable from the neutral distributions. Looking at the species scores more closely, it seems that the observed `lab` herbivory distribution has a much lower maximum herbivory and Lorenz asymmetry coefficient (lac) value compared to the neutral expectation. 

```{r}
# Reduce the dimension of probes with PCA and plot
vegan::rda(scale(probes_d[,probes])) %>%
  get_biplot(group = probes_d$source,
             group2 = probes_d$type,
             ellipse = "t",
             sites_size = ifelse(probes_d$type == "observed", 5, 2)) +
  ggplot2::scale_shape_manual(values = c(19,2)) + 
  ggplot2::labs(shape = "type", 
       color = "source")
```


Indeed, when we look at more holistic tests of distributions, we find that the observed `lab` distribution deviate from the neutral distribution significantly. 

```{r}
# Do other test of absolute fit
set.seed(123)
compare_dist_list(list("lab" = lab_herb_fit,
                       "field" = field_herb_fit), 
                  test = c("ks", "kl","ad"), 
                  silent = TRUE)
```


## Economic injury threshold

|            Here, I show how one can use the neutral herbivory model to inform pest management decisions. The motivation is that the injury threshold is often based on mean level of herbivory, but individual plants (the actual production unit) sustains different levels of herbivory and this leads to under or over estimation of yield. Suppose a crop (e.g. soybeans) has an injury threshold of 20%, we can use the distribution function to calculate the proportion of plants above the injury threshold. Let's assume that all plants have one leaf. 

```{r}
x <- seq(0.4,10, by = 0.4)
y <- vapply(x,
       function(x) palloT(0.2, 
                          lambda = x,
                          lower.tail = FALSE), 
       FUN.VALUE = numeric(1))

plot(y~x, 
     ylab = "Pr(Herbivory > 20%)", 
     xlab = expression(Attack~rate~lambda))
graphics::abline(v = 3.3, col = "red") 
graphics::segments(x0 = 0,x1 = 3.3, y0 = 0.3, y1 = 0.3, col = "black", lty = "dashed")

```

Here, I plotted the proportion of crop above the 20% injury threshold under different mean herbivory intensities. The red line corresponds to 20% mean herbivory. We can see that roughly a third of crops experience herbivory greater than the injury threshold if management action is only taken when the mean
herbivory reaches 20%.  


If we assume a yield function of $Yield = 100 -100 \phi_T^4$, we can calculate the yield gap due to overlooking the variation in herbivory individual plants sustain (i.e. non-linear averaging effect). 

```{r}
JE(model = function(x){100 -100*x^4}, 
   x = ralloT(n = 1000, lambda = 3.3), 
   plot = TRUE)

```



# References

Carmona, D., M. J. Lajeunesse, and M. T. J. Johnson. 2011. Plant traits that predict resistance to herbivores. Functional Ecology 25:358–367.  
Kendall, B. E., C. J. Briggs, W. W. Murdoch, P. Turchin, S. P. Ellner, E. McCauley, R. M. Nisbet, and S. N. Wood. 1999. Why Do Populations Cycle? A Synthesis of Statistical and Mechanistic Modeling Approaches. Ecology 80:1789–1805.  
Kubinec, R. 2022. Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds. Political Analysis:1–18.  
McGill, B. 2003. Strong and weak tests of macroecological theory. Oikos 102:679–685.  
McGill, B. J., B. A. Maurer, and M. D. Weiser. 2006. Empirical Evaluation of Neutral Theory. Ecology 87:1411–1423.  
Moller, A. P., and M. D. Jennions. 2002. How Much Variance Can Be Explained by Ecologists and Evolutionary Biologists? Oecologia 132:492–500.  
Ospina, R., and S. L. P. Ferrari. 2008. Inflated beta distributions. Statistical Papers 51:111.  
Platt, J. R. 1964. Strong Inference. Science 146:347–353.  
Smithson, M., and J. Verkuilen. 2006. A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables. Psychological Methods 11:54–71.  
Warton, D. I., and F. K. C. Hui. 2011. The arcsine is asinine: the analysis of proportions in ecology. Ecology 92:3–10.  




# Session Info

```{r}
herbivar.version()
sessionInfo()
```

















